import { NextResponse } from 'next/server';
import { GoogleGenAI } from '@google/genai';
import { GoogleAuth } from 'google-auth-library';

// --- Configura√ß√£o dos Clientes de IA ---
// NOTA: Isso requer vari√°veis de ambiente em .env.local ou Vercel:
// GEMINI_API_KEY = "sua-chave-api-gemini-aqui"
// VERTEX_AI_API_KEY = "sua-chave-api-vertex-ai-aqui" (nova - preferencial)
// GOOGLE_CLOUD_PROJECT = "seu-projeto-gcloud" (fallback se n√£o usar API Key)
// GOOGLE_CLOUD_LOCATION = "us-central1" (fallback se n√£o usar API Key)
// GOOGLE_APPLICATION_CREDENTIALS_JSON = "conte√∫do do JSON da service account" (fallback)
// ----------------------------------------------------

// Cliente Gemini (para An√°lise de Vis√£o)
const genAI = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY || '' });

// --- Helper para converter Arquivo (File) para base64 ---
async function fileToBase64(file: File): Promise<{ data: string; mimeType: string }> {
  const base64EncodedData = Buffer.from(await file.arrayBuffer()).toString('base64');
  return {
    data: base64EncodedData,
    mimeType: file.type,
  };
}

// --- Helper para redimensionar imagem para garantir dimens√µes compat√≠veis ---
async function resizeImageToMatch(
  imageBase64: string,
  targetWidth: number,
  targetHeight: number
): Promise<string> {
  // Importar dinamicamente sharp
  try {
    // eslint-disable-next-line @typescript-eslint/no-require-imports
    const sharp = require('sharp');
    const imageBuffer = Buffer.from(imageBase64, 'base64');
    const resized = await sharp(imageBuffer)
      .resize(targetWidth, targetHeight, {
        fit: 'fill',
        background: { r: 255, g: 255, b: 255, alpha: 1 }
      })
      .toBuffer();
    return resized.toString('base64');
  } catch {
    // Se sharp n√£o estiver dispon√≠vel, retornar original
    console.warn('‚ö†Ô∏è Sharp n√£o dispon√≠vel, usando imagem original. Instale: npm install sharp');
    return imageBase64;
  }
}

// --- Helper para obter dimens√µes de uma imagem ---
async function getImageDimensions(imageBase64: string): Promise<{ width: number; height: number }> {
  try {
    // eslint-disable-next-line @typescript-eslint/no-require-imports
    const sharp = require('sharp');
    const imageBuffer = Buffer.from(imageBase64, 'base64');
    const metadata = await sharp(imageBuffer).metadata();
    return {
      width: metadata.width || 1024,
      height: metadata.height || 1024,
    };
  } catch {
    // Se sharp n√£o estiver dispon√≠vel, retornar dimens√µes padr√£o
    console.warn('‚ö†Ô∏è N√£o foi poss√≠vel obter dimens√µes, usando padr√£o 1024x1024');
    return { width: 1024, height: 1024 };
  }
}

// --- Helper para obter token de acesso do Google Cloud ---
async function getAccessToken(): Promise<string> {
  let auth: GoogleAuth;
  
  // Se temos credenciais JSON (Service Account da Vercel), usar elas
  if (process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON) {
    try {
      const credentials = JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON);
      auth = new GoogleAuth({
        credentials,
        scopes: ['https://www.googleapis.com/auth/cloud-platform'],
      });
    } catch (error) {
      throw new Error(`Erro ao parsear GOOGLE_APPLICATION_CREDENTIALS_JSON: ${error instanceof Error ? error.message : String(error)}`);
    }
  } else {
    // Tentar usar Application Default Credentials (para desenvolvimento local)
    auth = new GoogleAuth({
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });
  }

  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();
  
  if (!accessToken.token) {
    throw new Error('N√£o foi poss√≠vel obter token de acesso. Verifique se GOOGLE_APPLICATION_CREDENTIALS_JSON est√° configurado corretamente na Vercel.');
  }
  
  return accessToken.token;
}

// --- Tipos para resposta do Imagen ---
interface ImagenPrediction {
  bytesBase64Encoded?: string;
  imageBytes?: string;
  generatedImage?: {
    bytesBase64Encoded?: string;
    imageBytes?: string;
  };
}

interface ImagenResponse {
  predictions?: ImagenPrediction[];
}

// --- Helper para processar resposta do Imagen ---
async function processImagenResponse(imagenData: unknown): Promise<NextResponse> {
  console.log('üì¶ Processando resposta do Vertex AI Imagen');
  
  // Log completo da resposta (limitado a 2000 chars para n√£o sobrecarregar)
  const responseStr = JSON.stringify(imagenData, null, 2);
  console.log('üìã Estrutura completa da resposta (primeiros 2000 chars):', responseStr.substring(0, 2000));
  
  // A resposta pode ter diferentes estruturas, tentar todas
  const response = imagenData as Record<string, unknown>;
  
  // Tentar diferentes caminhos na resposta
  let predictions: ImagenPrediction[] = [];
  
  // Caminho 1: predictions no topo
  if (Array.isArray(response.predictions)) {
    predictions = response.predictions as ImagenPrediction[];
    console.log('‚úÖ Encontrado: predictions no topo');
  }
  // Caminho 2: predictions dentro de data
  else if (response.data) {
    const data = response.data as Record<string, unknown>;
    if (Array.isArray(data.predictions)) {
      predictions = data.predictions as ImagenPrediction[];
      console.log('‚úÖ Encontrado: predictions dentro de data');
    } else if (Array.isArray(data)) {
      predictions = data as ImagenPrediction[];
      console.log('‚úÖ Encontrado: data √© um array direto');
    }
  }
  // Caminho 3: resposta direta √© um array (mas precisa ter conte√∫do)
  else if (Array.isArray(response) && response.length > 0) {
    predictions = response as ImagenPrediction[];
    console.log('‚úÖ Encontrado: resposta √© um array direto');
  }
  // Caminho 3b: resposta √© um array vazio (problema)
  else if (Array.isArray(response) && response.length === 0) {
    console.error('‚ùå Resposta √© um array vazio!');
    throw new Error('A resposta do Imagen √© um array vazio. Isso pode indicar que o modelo n√£o gerou nenhuma imagem ou houve um erro no processamento.');
  }
  // Caminho 4: imagem direta na resposta (sem array)
  else if (response.bytesBase64Encoded || response.imageBytes || response.image) {
    predictions = [response as ImagenPrediction];
    console.log('‚úÖ Encontrado: imagem direta na resposta');
  }
  // Caminho 5: verificar se h√° algum campo que contenha a imagem
  else {
    // Tentar encontrar qualquer campo que possa conter a imagem
    for (const [key, value] of Object.entries(response)) {
      if (value && typeof value === 'object') {
        const obj = value as Record<string, unknown>;
        if (obj.bytesBase64Encoded || obj.imageBytes || obj.image || (Array.isArray(value) && value.length > 0)) {
          if (Array.isArray(value)) {
            predictions = value as ImagenPrediction[];
          } else {
            predictions = [obj as ImagenPrediction];
          }
          console.log(`‚úÖ Encontrado: imagem no campo "${key}"`);
          break;
        }
      }
    }
  }
  
  if (predictions.length === 0) {
    console.error('‚ùå Estrutura da resposta n√£o reconhecida');
    console.error('üìã Chaves dispon√≠veis:', Object.keys(response));
    console.error('üìã Resposta completa:', responseStr.substring(0, 1000));
    throw new Error(`A resposta do Imagen n√£o cont√©m predictions. Estrutura recebida: ${JSON.stringify(Object.keys(response))}. Resposta completa (primeiros 500 chars): ${responseStr.substring(0, 500)}`);
  }

  // A estrutura pode variar, tentar diferentes formatos
  let imageBase64: string | undefined;
  const firstPrediction = predictions[0] as Record<string, unknown>;
  
  if (typeof firstPrediction.bytesBase64Encoded === 'string') {
    imageBase64 = firstPrediction.bytesBase64Encoded;
  } else if (typeof firstPrediction.imageBytes === 'string') {
    imageBase64 = firstPrediction.imageBytes;
  } else if (firstPrediction.generatedImage) {
    const generatedImage = firstPrediction.generatedImage as Record<string, unknown>;
    imageBase64 = (generatedImage.bytesBase64Encoded || generatedImage.imageBytes) as string | undefined;
  } else if (typeof firstPrediction.image === 'string') {
    imageBase64 = firstPrediction.image;
  }

  if (!imageBase64) {
    console.error('‚ùå Estrutura da prediction n√£o reconhecida:', JSON.stringify(firstPrediction, null, 2));
    throw new Error(`A resposta do Imagen n√£o cont√©m uma imagem gerada no formato esperado. Chaves dispon√≠veis: ${JSON.stringify(Object.keys(firstPrediction))}`);
  }

  const imageBytes = Buffer.from(imageBase64, 'base64');
  console.log('‚úÖ Inpainting conclu√≠do com sucesso!');

  return new NextResponse(imageBytes, {
    status: 200,
    headers: {
      'Content-Type': 'image/png',
    },
  });
}

// --- A Rota da API (POST) ---
export async function POST(request: Request) {
  try {
    // 1. Ler os 3 arquivos do FormData (Contrato 6.2)
    const formData = await request.formData();
    const friendImageFile = formData.get('friendImage') as File | null;
    const baseImageFile = formData.get('baseImage') as File | null;
    const maskImageFile = formData.get('maskImage') as File | null;

    if (!friendImageFile || !baseImageFile || !maskImageFile) {
      return NextResponse.json(
        { error: 'Arquivos ausentes. (friendImage, baseImage, maskImage s√£o obrigat√≥rios)' },
        { status: 400 },
      );
    }

    // --- ETAPA DE IA 1: An√°lise de Vis√£o (Gemini) ---
    // (Descrever a friendImage para criar o prompt de inpainting)

    console.log('Iniciando Etapa 1: An√°lise de Vis√£o (Gemini)');
    const friendImageBase64 = await fileToBase64(friendImageFile);
    const visionPrompt =
      'Descreva esta pessoa em detalhes objetivos para uma IA de gera√ß√£o de imagem. Foque em: sexo, idade aproximada, etnia, cor e estilo do cabelo, pelos faciais (barba/bigode), √≥culos e quaisquer caracter√≠sticas marcantes. Seja conciso e direto. Responda apenas com a descri√ß√£o.';

    // Tentar diferentes modelos em ordem de prefer√™ncia
    const modelsToTry = [
      'gemini-2.0-flash-exp',
      'gemini-1.5-flash',
      'gemini-1.5-pro',
      'gemini-1.5-flash-002',
      'gemini-1.5-pro-002',
    ];
    
    let textPrompt = '';
    let lastError: Error | null = null;

    console.log('üîç Iniciando tentativas com modelos Gemini...');
    for (const modelName of modelsToTry) {
      try {
        console.log(`üîÑ Tentando modelo: ${modelName}`);
        const visionResponse = await genAI.models.generateContent({
          model: modelName,
          contents: [
            {
              role: 'user',
              parts: [
                { text: visionPrompt },
                {
                  inlineData: {
                    data: friendImageBase64.data,
                    mimeType: friendImageBase64.mimeType,
                  },
                },
              ],
            },
          ],
        });

        const candidates = visionResponse.candidates || [];
        if (candidates.length > 0) {
          const parts = candidates[0].content?.parts || [];
          for (const part of parts) {
            if ('text' in part && part.text) {
              textPrompt = part.text;
              break;
            }
          }
        }

        if (textPrompt && textPrompt.trim() !== '') {
          console.log(`‚úÖ Modelo ${modelName} funcionou com sucesso!`);
          break;
        }
      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error));
        console.log(`‚ùå Modelo ${modelName} falhou: ${lastError.message}`);
        continue;
      }
    }

    if (!textPrompt || textPrompt.trim() === '') {
      return NextResponse.json(
        { error: `N√£o foi poss√≠vel analisar a imagem do amigo. √öltimo erro: ${lastError?.message || 'Nenhum modelo dispon√≠vel'}` },
        { status: 500 },
      );
    }

    // Este √© o prompt que ser√° usado para "pintar" o amigo na cena
    const finalInpaintingPrompt = `FOTO: ${textPrompt}, em um cen√°rio com um pol√≠tico, fotorrealista.`;
    console.log('Etapa 1 Conclu√≠da. Prompt Gerado:', finalInpaintingPrompt);


    // --- ETAPA DE IA 2: Inpainting Real (Vertex AI Imagen via REST API) ---
    // (Usar Vertex AI Imagen para fazer inpainting real com m√°scara, preservando identidade)

    console.log('Iniciando Etapa 2: Inpainting Real (Vertex AI Imagen)');
    
    // Preparar as imagens em base64 (friendImageBase64 j√° foi criado na Etapa 1)
    const baseImageBase64 = await fileToBase64(baseImageFile);
    let maskImageBase64 = await fileToBase64(maskImageFile);

    // Obter dimens√µes da imagem base e redimensionar a m√°scara para corresponder
    try {
      const baseDimensions = await getImageDimensions(baseImageBase64.data);
      console.log(`üìê Dimens√µes da imagem base: ${baseDimensions.width}x${baseDimensions.height}`);
      
      const maskDimensions = await getImageDimensions(maskImageBase64.data);
      console.log(`üìê Dimens√µes da m√°scara: ${maskDimensions.width}x${maskDimensions.height}`);
      
      // Se as dimens√µes n√£o corresponderem, redimensionar a m√°scara
      if (maskDimensions.width !== baseDimensions.width || maskDimensions.height !== baseDimensions.height) {
        console.log(`üîÑ Redimensionando m√°scara de ${maskDimensions.width}x${maskDimensions.height} para ${baseDimensions.width}x${baseDimensions.height}`);
        const resizedMaskData = await resizeImageToMatch(
          maskImageBase64.data,
          baseDimensions.width,
          baseDimensions.height
        );
        maskImageBase64 = {
          ...maskImageBase64,
          data: resizedMaskData,
        };
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è N√£o foi poss√≠vel verificar/redimensionar imagens:', error);
      // Continuar mesmo assim - o Vertex AI pode lidar com isso ou retornar erro mais claro
    }

    // Criar prompt de inpainting que preserva a identidade
    const inpaintingPrompt = `${finalInpaintingPrompt}. A pessoa deve aparecer EXATAMENTE como na imagem de refer√™ncia, mantendo todas as caracter√≠sticas faciais id√™nticas.`;

    try {
      // Tentar primeiro com API Key (mais simples)
      const vertexAIApiKey = process.env.VERTEX_AI_API_KEY;
      
      if (vertexAIApiKey) {
        console.log('üîë Usando Vertex AI API Key...');
        
        // Tentar diferentes endpoints para Imagen com API Key
        const imagenEndpoints = [
          'https://aiplatform.googleapis.com/v1/publishers/google/models/imagegeneration@006:predict',
          'https://aiplatform.googleapis.com/v1/publishers/google/models/imagegeneration@005:predict',
          'https://aiplatform.googleapis.com/v1/publishers/google/models/imagen-3:predict',
          'https://aiplatform.googleapis.com/v1/publishers/google/models/imagen:predict',
        ];


        for (const endpoint of imagenEndpoints) {
          try {
            const fullEndpoint = `${endpoint}?key=${vertexAIApiKey}`;
            console.log(`üì° Tentando endpoint: ${endpoint}`);

            const requestBody = {
              instances: [
                {
                  prompt: inpaintingPrompt,
                  image: {
                    bytesBase64Encoded: baseImageBase64.data,
                  },
                  mask: {
                    image: {
                      bytesBase64Encoded: maskImageBase64.data,
                    },
                  },
                  referenceImage: {
                    bytesBase64Encoded: friendImageBase64.data,
                  },
                },
              ],
              parameters: {
                sampleCount: 1,
                guidanceScale: 12,
                aspectRatio: '1:1',
              },
            };

            console.log('üì§ Enviando requisi√ß√£o para Imagen');
            console.log('üìù Prompt:', inpaintingPrompt.substring(0, 200));
            console.log('üñºÔ∏è Tamanho imagem base:', baseImageBase64.data.length, 'chars');
            console.log('üé≠ Tamanho m√°scara:', maskImageBase64.data.length, 'chars');
            console.log('üë§ Tamanho refer√™ncia:', friendImageBase64.data.length, 'chars');

            const imagenResponse = await fetch(fullEndpoint, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify(requestBody),
            });

            if (!imagenResponse.ok) {
              const errorText = await imagenResponse.text();
              console.error(`‚ùå Endpoint falhou (${imagenResponse.status}):`, errorText);
              
              if (imagenResponse.status === 403 || imagenResponse.status === 404) {
                continue; // Tentar pr√≥ximo endpoint
              }
              
              throw new Error(`Vertex AI retornou erro ${imagenResponse.status}: ${errorText}`);
            }

            const responseData = await imagenResponse.json();
            console.log(`‚úÖ Endpoint funcionou com API Key!`);
            console.log('üìã Resposta recebida:', JSON.stringify(responseData, null, 2).substring(0, 500)); // Log parcial para debug
            return await processImagenResponse(responseData);
            
          } catch (error) {
            if (error instanceof TypeError || (error instanceof Error && error.message.includes('fetch'))) {
              continue; // Tentar pr√≥ximo endpoint
            }
            throw error;
          }
        }

        // Se nenhum endpoint funcionou com API Key, tentar Service Account como fallback
        console.log('‚ö†Ô∏è API Key n√£o funcionou, tentando Service Account como fallback...');
      }

      // Fallback: usar Service Account (c√≥digo original)
      console.log('üîë Usando Service Account (fallback)...');
      
      const projectId = process.env.GOOGLE_CLOUD_PROJECT;
      const location = process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';
      
      if (!projectId) {
        throw new Error('GOOGLE_CLOUD_PROJECT n√£o configurado. Configure VERTEX_AI_API_KEY ou GOOGLE_CLOUD_PROJECT nas vari√°veis de ambiente.');
      }

      const accessToken = await getAccessToken();
      console.log('‚úÖ Token obtido com sucesso');

      // Usar Service Account com endpoint baseado em projeto
      const modelVersions = [
        'imagegeneration@006',
        'imagegeneration@005',
        'imagegeneration@004',
        'imagegeneration@003',
      ];

      let imagenError: Error | null = null;
      let imagenData: ImagenResponse | null = null;

      for (const modelVersion of modelVersions) {
        const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${modelVersion}:predict`;
        
        console.log(`üì° Tentando modelo: ${modelVersion}`);

        const requestBody = {
          instances: [
            {
              prompt: inpaintingPrompt,
              image: {
                bytesBase64Encoded: baseImageBase64.data,
              },
              mask: {
                image: {
                  bytesBase64Encoded: maskImageBase64.data,
                },
              },
              referenceImage: {
                bytesBase64Encoded: friendImageBase64.data,
              },
            },
          ],
          parameters: {
            sampleCount: 1,
            guidanceScale: 12,
            aspectRatio: '1:1',
          },
        };

        try {
          const imagenResponse = await fetch(endpoint, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${accessToken}`,
              'Content-Type': 'application/json',
            },
            body: JSON.stringify(requestBody),
          });

          if (!imagenResponse.ok) {
            const errorText = await imagenResponse.text();
            console.error(`‚ùå Modelo ${modelVersion} falhou (${imagenResponse.status}):`, errorText);
            
            if (imagenResponse.status === 403 || imagenResponse.status === 404) {
              imagenError = new Error(`Modelo ${modelVersion}: ${errorText}`);
              continue;
            }
            
            throw new Error(`Vertex AI retornou erro ${imagenResponse.status}: ${errorText}`);
          }

          const responseData = await imagenResponse.json();
          console.log(`‚úÖ Modelo ${modelVersion} funcionou!`);
          console.log('üìã Resposta recebida:', JSON.stringify(responseData, null, 2).substring(0, 500)); // Log parcial para debug
          imagenData = responseData;
          break;
          
        } catch (error) {
          if (error instanceof TypeError || (error instanceof Error && error.message.includes('fetch'))) {
            imagenError = error;
            continue;
          }
          throw error;
        }
      }

      // Se nenhum modelo funcionou
      if (!imagenData) {
        throw new Error(`Nenhum modelo do Vertex AI Imagen est√° dispon√≠vel. √öltimo erro: ${imagenError?.message || 'Desconhecido'}. Configure VERTEX_AI_API_KEY ou verifique as permiss√µes da Service Account.`);
      }

      return await processImagenResponse(imagenData);

    } catch (error) {
      console.error('‚ùå Erro no inpainting:', error);
      return NextResponse.json(
        { error: `Erro no inpainting: ${error instanceof Error ? error.message : String(error)}` },
        { status: 500 },
      );
    }

  } catch (error) {
    console.error('Erro grave na API /api/generate:', error);
    const errorMessage = error instanceof Error ? error.message : 'Erro interno do servidor.';
    return NextResponse.json(
      { error: errorMessage },
      { status: 500 },
    );
  }
}
